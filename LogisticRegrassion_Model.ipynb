{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiqDTxCETaaT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Sample data\n",
        "# Assuming df is your DataFrame containing \"sentence\" and \"label\" columns\n",
        "X = df[\"sentence\"].values\n",
        "y = df[\"label\"].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize input sentences and format them as tensors\n",
        "def tokenize_data(sentences, labels):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sent in sentences:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                            sent,\n",
        "                            add_special_tokens=True,\n",
        "                            max_length=128,\n",
        "                            padding='max_length',\n",
        "                            truncation=True,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt'\n",
        "                       )\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels_tensor = torch.tensor(labels)\n",
        "    return input_ids, attention_masks, labels_tensor\n",
        "\n",
        "train_inputs, train_masks, train_labels = tokenize_data(X_train, y_train)\n",
        "test_inputs, test_masks, test_labels = tokenize_data(X_test, y_test)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "# Fine-tune BERT\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "\n",
        "epochs = 4\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    model.eval()\n",
        "    val_accuracy = 0\n",
        "    for batch in test_dataloader:\n",
        "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        logits = outputs.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        val_accuracy += accuracy_score(label_ids, np.argmax(logits, axis=1))\n",
        "    avg_val_accuracy = val_accuracy / len(test_dataloader)\n",
        "    print(f'Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss}, Validation Accuracy: {avg_val_accuracy}')\n",
        "\n",
        "# Tokenize text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "logistic_regression = LogisticRegression(max_iter=1000)\n",
        "logistic_regression.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_tfidf = logistic_regression.score(X_test_tfidf, y_test)\n",
        "print(\"Accuracy after hyperparameter tuning:\", accuracy_tfidf)"
      ]
    }
  ]
}